# -*- coding: utf-8 -*-
"""Pytorch25.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z7mO3u1eaS2kMtI4nsP7t404eCO1wT-R
"""

import tensorflow as tf
import cv2
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from pandas import read_csv
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, balanced_accuracy_score
from sklearn.utils import class_weight
import math
import gc

model_name = "model_large"
img_width = 200
img_height = 66
img_channels = 3
model_file = "models/{}-{}x{}x{}".format(model_name[6:], img_width, img_height, img_channels)

"""# New Section"""

import requests 
import os
import zipfile
import shutil

shutil.rmtree('dataset', True)
shutil.rmtree('models', True)

dataset_dir = "dataset"
dataset_zipfile = 'Dataset.zip'
RunningInCOLAB = 'google.colab' in str(get_ipython())
force_unzip = True

if RunningInCOLAB: 
    print("in colab")
    from google.colab import files
    uploaded = files.upload()
    for fn in uploaded.keys():
        print('User uploaded file "{name}" with length {length} bytes'.format(
            name=fn, length=len(uploaded[fn])))
        dataset_zipfile = fn  # expecting a zip file to be uploaded. 
else:
    print ("not in colab")
    # RPI_IP=None # replace this with the IP address of your raspberry pi 
    # if RPI_IP is not None:
    #     URL="http://"+RPI_IP+":8000/"+dataset_zipfile
    #     filename=URL.split('/')[-1]
    #     r = requests.get(URL)
    #     open(filename, 'wb').write(r.content)

if not os.path.isfile(dataset_zipfile):
    default_dataset_URL = "https://raw.githubusercontent.com/heechul/DeepPicar-v3/devel/Dataset-kucsl-Apr2022.zip"    
    print ("No existing dataset. download from ", default_dataset_URL)
    r = requests.get(default_dataset_URL)
    open(dataset_zipfile, 'wb').write(r.content)

if not os.path.isdir(dataset_dir) or force_unzip==True:
    print ('unzip %s into %s folder' % (dataset_zipfile, dataset_dir))
    zip_ref = zipfile.ZipFile(dataset_zipfile)
    zip_ref.extractall(dataset_dir)

# Train/test data lists
imgs = []
vals = []

import glob
for vid_file_path in glob.iglob(f'{dataset_dir}/*.avi'):
    csv_file_path = vid_file_path.replace('video', 'key').replace('avi', 'csv')
    print(vid_file_path, csv_file_path)

    vid = cv2.VideoCapture(vid_file_path)
    ret,img = vid.read()
    while(ret):
        img =  img[img.shape[0]//2:]
        # Convert to grayscale and readd channel dimension
        if img_channels == 1:
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            img = cv2.resize(img, (img_width, img_height))
            #img = np.reshape(img, (img_height, img_width, img_channels))
            img = np.reshape(img, (img_channels, img_height, img_width))

        # For RGB, just need to resize image
        else:
            img = cv2.resize(img, (img_width, img_height))
            img = np.reshape(img, (img_channels, img_height, img_width))
        img = img / 255.
        imgs.append(img)
        ret,img = vid.read()
    df = read_csv(csv_file_path)
    temp = np.asarray(df["wheel"].values)
    vals.extend(temp)
    print(len(imgs), len(vals))    

# Convert lists to numpy arrays and ensure they are of equal length    
imgs = np.asarray(imgs)  # input images
vals = np.asarray(vals)  # steering angles
assert len(imgs) == len(vals)
print("Loaded {} samples".format(len(imgs)))

import torch
from torch.utils.data import DataLoader, TensorDataset

# Split the data
x_train, x_test, y_train, y_test = train_test_split(imgs, vals, test_size=0.35, stratify=vals)
 #new
del vals
print(len(x_train))
print(len(x_test))
print(len(y_train))
print(x_test.shape,x_train.shape)
gc.collect()
gc.collect()



# Convert numpy arrays to PyTorch tensors
x_train = torch.from_numpy(x_train).float()
y_train = torch.from_numpy(y_train).float()

x_test = torch.from_numpy(x_test).float()
y_test = torch.from_numpy(y_test).float()

# Create PyTorch datasets
train_dataset = TensorDataset(x_train, y_train)
test_dataset = TensorDataset(x_test, y_test)

# Create PyTorch data loaders
batch_size = 64

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

import torch.nn as nn

# Define the model
class Model(nn.Module):
    def __init__(self, img_height, img_width, img_channels):
        super(Model, self).__init__()
        
        # Define the convolutional layers
        self.conv1 = nn.Conv2d(img_channels, 24, kernel_size=5, stride=2)
        self.conv2 = nn.Conv2d(24, 36, kernel_size=5, stride=2)
        self.conv3 = nn.Conv2d(36, 48, kernel_size=5, stride=2)
        self.conv4 = nn.Conv2d(48, 64, kernel_size=3)
        self.conv5 = nn.Conv2d(64, 64, kernel_size=3)
        
        # Define the fully connected layers
        self.fc1 = nn.Linear(self._calculate_fc_input_size(img_height, img_width, img_channels), 100)
        self.fc2 = nn.Linear(100, 50)
        self.fc3 = nn.Linear(50, 10)
        self.fc4 = nn.Linear(10, 1)
        
    def forward(self, x):
        # Pass the input through the convolutional layers
        x = nn.functional.relu(self.conv1(x))
        x = nn.functional.relu(self.conv2(x))
        x = nn.functional.relu(self.conv3(x))
        x = nn.functional.relu(self.conv4(x))
        x = nn.functional.relu(self.conv5(x))
        
        # Flatten the output and pass it through the fully connected layers
        x = x.view(x.size(0), -1)
        x = nn.functional.relu(self.fc1(x))
        x = nn.functional.relu(self.fc2(x))
        x = nn.functional.relu(self.fc3(x))
        x = nn.functional.tanh(self.fc4(x))
        
        return x
        
    def _calculate_fc_input_size(self, img_height, img_width, img_channels):
        # Calculate the output size of the convolutional layers
        with torch.no_grad():
            x = torch.zeros((1, img_channels, img_height, img_width))
            x = self.conv5(self.conv4(self.conv3(self.conv2(self.conv1(x)))))
            conv_output_size = x.view(1, -1).size(1)
        return conv_output_size

import torch.optim as optim
model = Model(img_height,img_width,img_channels)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=3e-4)
if os.path.isfile(model_file+".pt"):
    model.load_state_dict(torch.load(model_file+".pt"))
    optimizer = optim.Adam(model.parameters(), lr=5e-5)
criterion = nn.SmoothL1Loss()
#model.to(device)
model.train()

import torch
import torch.nn as nn
import torch.optim as optim

print("Model Fit")
def fit(model, train_loader, val_loader, epochs, lr):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    train_losses = []
    val_losses = []
    
    for epoch in range(epochs):
        train_loss = 0.0
        val_loss = 0.0
        
        # Training loop
        model.train()
        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            targets = targets.reshape(-1, 1)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * inputs.size(0)
        train_loss /= len(train_loader.dataset)
        #train_loss /= len(train_loader)
        train_losses.append(train_loss)
        
        # Validation loop
        model.eval()
        with torch.no_grad():
            for inputs, targets in val_loader:
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                val_loss += loss.item() * inputs.size(0)
            val_loss /= len(val_loader.dataset)
            #val_loss /= len(val_loader)
            val_losses.append(val_loss)
            
        # Print training/validation loss for each epoch
        print(f"Epoch {epoch+1}/{epochs}: Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
        
    return train_losses, val_losses

# Example usage
'''train_loader = # your train data loader
val_loader = # your validation data loade'''
model = Model(img_height, img_width, img_channels)
epochs = 10
lr = 0.0001

train_losses, val_losses = fit(model, train_loader, test_loader, epochs, lr)



import matplotlib.pyplot as plt

# Plot training and validation losses
plt.plot(train_losses)
plt.plot(val_losses)
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

